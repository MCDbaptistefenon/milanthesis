{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11tPW0jp70K1bNO19NxyIZzZOBdDyHpQD",
      "authorship_tag": "ABX9TyPFYat2IrkWzr2YtdNC/uMh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCDbaptistefenon/milanthesis/blob/main/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "IKtwN3q0Qzgk",
        "outputId": "c7be1aec-d4d8-41bf-f2a9-afd433b9020f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8293031e-e6b6-4aa4-9e08-266b66763e40\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8293031e-e6b6-4aa4-9e08-266b66763e40\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fixed_project.zip to fixed_project.zip\n",
            "Archive:  fixed_project.zip\n",
            "   creating: fixed_project/\n",
            "  inflating: fixed_project/run_hybrid.sh  \n",
            "  inflating: fixed_project/requirements.txt  \n",
            "  inflating: fixed_project/run_one.sh  \n",
            "  inflating: fixed_project/config.yaml  \n",
            "  inflating: fixed_project/run_behav.sh  \n",
            "  inflating: fixed_project/visualize.py  \n",
            "  inflating: fixed_project/train_all.sh  \n",
            "  inflating: fixed_project/README.md  \n",
            "   creating: fixed_project/src/\n",
            "  inflating: fixed_project/src/acquire.py  \n",
            "  inflating: fixed_project/src/preprocess.py  \n",
            "  inflating: fixed_project/src/model.py  \n",
            "   creating: fixed_project/src/viz/\n",
            "  inflating: fixed_project/src/viz/bar_charts.py  \n",
            "  inflating: fixed_project/src/viz/__init__.py  \n",
            "  inflating: fixed_project/src/viz/utils.py  \n",
            "  inflating: fixed_project/src/viz/box_plot.py  \n",
            "  inflating: fixed_project/src/train.py  \n",
            "  inflating: fixed_project/src/evaluate.py  \n",
            "/content/fixed_project\n"
          ]
        }
      ],
      "source": [
        "# Upload the zip file to Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select fixed_project.zip when prompted\n",
        "\n",
        "# Extract the zip file\n",
        "!unzip fixed_project.zip\n",
        "%cd fixed_project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R5wejeuiQ-vT",
        "outputId": "0d23e763-42a8-4f96-98f7-478e64bba83e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio>=0.13.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.6.0+cu124)\n",
            "Collecting transformers==4.30.2 (from -r requirements.txt (line 4))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.5.3 (from -r requirements.txt (line 5))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting networkx==3.1 (from -r requirements.txt (line 8))\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting tqdm==4.65.0 (from -r requirements.txt (line 9))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.7.1 (from -r requirements.txt (line 10))\n",
            "  Downloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting seaborn==0.12.2 (from -r requirements.txt (line 11))\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: spacy>=3.5.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.8.5)\n",
            "Collecting emoji==2.2.0 (from -r requirements.txt (line 13))\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting newspaper3k==0.2.8 (from -r requirements.txt (line 14))\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting optuna==3.1.1 (from -r requirements.txt (line 15))\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting kaggle==1.5.13 (from -r requirements.txt (line 16))\n",
            "  Downloading kaggle-1.5.13.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2->-r requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 10)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 10)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 10)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 10)) (3.2.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k==0.2.8->-r requirements.txt (line 14)) (4.13.4)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k==0.2.8->-r requirements.txt (line 14)) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k==0.2.8->-r requirements.txt (line 14)) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alembic>=1.5.0 (from optuna==3.1.1->-r requirements.txt (line 15))\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting cmaes>=0.9.1 (from optuna==3.1.1->-r requirements.txt (line 15))\n",
            "  Downloading cmaes-0.11.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting colorlog (from optuna==3.1.1->-r requirements.txt (line 15))\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from optuna==3.1.1->-r requirements.txt (line 15)) (2.0.40)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.13->-r requirements.txt (line 16)) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.13->-r requirements.txt (line 16)) (2025.4.26)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.13->-r requirements.txt (line 16)) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.13->-r requirements.txt (line 16)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (0.15.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (2.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.5.3->-r requirements.txt (line 12)) (3.5.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna==3.1.1->-r requirements.txt (line 15)) (1.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k==0.2.8->-r requirements.txt (line 14)) (2.7)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.5.3->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k==0.2.8->-r requirements.txt (line 14)) (8.1.8)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.5.3->-r requirements.txt (line 12)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.5.3->-r requirements.txt (line 12)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.5.3->-r requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.3.0->optuna==3.1.1->-r requirements.txt (line 15)) (3.2.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.5.3->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.5.3->-r requirements.txt (line 12)) (0.1.5)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k==0.2.8->-r requirements.txt (line 14))\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.5.3->-r requirements.txt (line 12)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.5.3->-r requirements.txt (line 12)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.5.3->-r requirements.txt (line 12)) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.5.3->-r requirements.txt (line 12)) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.1->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle==1.5.13->-r requirements.txt (line 16)) (1.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.5.3->-r requirements.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.5.3->-r requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.5.3->-r requirements.txt (line 12)) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.5.3->-r requirements.txt (line 12)) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.5.3->-r requirements.txt (line 12)) (0.1.2)\n",
            "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmaes-0.11.1-py3-none-any.whl (35 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: emoji, kaggle, tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234910 sha256=098312ecc1a2aa6be02e5e7a9a37e3fa52c10588a1a3266e8c0569d1ec1761a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d5/63/4dbdee6f4e23f24b771ea5ac6c9ebe3d7e227028c60e06ead3\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.13-py3-none-any.whl size=77711 sha256=3d6e8d255647717a0ea6006f8ad4106660eb871f931669eb3ba0f0b75935c41f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/b2/6ae87ab801289eabd078a52cc8167e232e98f09b0e6cd2ca48\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=de4bfd781e224de340cf1f359ea2b08cdd7114873e9bfc103085d302d7153e52\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=41d4d94309a34b5316f137d479395605237a9b1f9bdf2adc32168cd0c401a182\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=34d8740b0416d27d685b3ee70b5b0dc7ba17bb76ea9e49a718312f3b0aca32fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=c639efe6b19d5ab44e1e0ae7291dc93c60ab028098b6fb6c452491c51aff2b48\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built emoji kaggle tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tokenizers, tinysegmenter, sgmllib3k, jieba3k, tqdm, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, feedparser, emoji, cssselect, colorlog, cmaes, scikit-learn, requests-file, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, kaggle, feedfinder2, alembic, transformers, tldextract, seaborn, optuna, nvidia-cusolver-cu12, newspaper3k\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.7.4.2\n",
            "    Uninstalling kaggle-1.7.4.2:\n",
            "      Successfully uninstalled kaggle-1.7.4.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.2\n",
            "    Uninstalling seaborn-0.13.2:\n",
            "      Successfully uninstalled seaborn-0.13.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dataproc-spark-connect 0.7.2 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alembic-1.15.2 cmaes-0.11.1 colorlog-6.9.0 cssselect-1.3.0 emoji-2.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 kaggle-1.5.13 matplotlib-3.7.1 networkx-3.1 newspaper3k-0.2.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optuna-3.1.1 pandas-1.5.3 requests-file-2.1.0 scikit-learn-1.2.2 seaborn-0.12.2 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "77b8ba9f9e63461182162d1f7266dbbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Print current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# List all files in the current directory to see what's available\n",
        "print(\"\\nFiles in current directory:\")\n",
        "!ls -la\n",
        "\n",
        "# Try to navigate to the correct directory structure\n",
        "if os.path.exists(\"fixed_project\"):\n",
        "    print(\"\\nFound fixed_project directory, changing to it...\")\n",
        "    os.chdir(\"fixed_project\")\n",
        "    print(\"New working directory:\", os.getcwd())\n",
        "    print(\"\\nFiles in fixed_project:\")\n",
        "    !ls -la"
      ],
      "metadata": {
        "id": "bC0Y7zEeShqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for Kaggle API token\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Upload the kaggle.json file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select the kaggle.json file when prompted\n",
        "\n",
        "# Move the uploaded file to the correct location\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "Mi1knKsgS--I",
        "outputId": "17b9aef3-f7ff-4e17-cd25-9c503c52eeea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b81b741b-c11c-4c5b-acfb-e29c25b44220\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b81b741b-c11c-4c5b-acfb-e29c25b44220\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data directories\n",
        "!mkdir -p data\n",
        "\n",
        "# Download ISOT dataset (main dataset)\n",
        "# Run the Python files directly\n",
        "!python src/acquire.py --dataset isot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHZDEY4MRbdY",
        "outputId": "1e3b961e-ccd2-411e-906f-5bf953ffe205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Downloading ISOT Fake News dataset\n",
            "[INFO] Downloading ISOT Fake News dataset\n",
            "[INFO] ISOT dataset downloaded to data/ISOT\n",
            "[INFO] All requested datasets have been downloaded\n",
            "[INFO] Next steps:\n",
            "1. Run 'python -m src.preprocess' to prepare the data\n",
            "2. Run './train_all.sh' to train and evaluate all models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the required packages if they're not already installed\n",
        "!pip install emoji networkx spacy tqdm matplotlib seaborn scikit-learn\n",
        "\n",
        "# Download spaCy model if not already downloaded\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Create the necessary directories\n",
        "!mkdir -p data/processed/graphs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Y8OK2fTq4c",
        "outputId": "34ff1a9b-2045-4e37-d919-b998cd1be49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.24.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting pandas>=0.25 (from seaborn)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 pandas-2.2.3\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the preprocessing script\n",
        "!python preprocess_real.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10DKB8zjTKdw",
        "outputId": "3ea87572-813d-4897-f791-a553d59b2f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/fixed_project/preprocess_real.py\", line 10, in <module>\n",
            "    from sklearn.model_selection import StratifiedKFold\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\", line 82, in <module>\n",
            "    from .base import clone\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 17, in <module>\n",
            "    from .utils import _IS_32BIT\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\", line 19, in <module>\n",
            "    from .murmurhash import murmurhash3_32\n",
            "  File \"sklearn/utils/murmurhash.pyx\", line 1, in init sklearn.utils.murmurhash\n",
            "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's clean up any previous attempts\n",
        "!rm -rf fixed_project preprocess_real.py minimal_project\n",
        "\n",
        "# Create a clean directory structure\n",
        "!mkdir -p fixed_project/src\n",
        "!mkdir -p data/ISOT\n",
        "!mkdir -p data/processed/graphs"
      ],
      "metadata": {
        "id": "_Q44tm-aVdKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install compatible versions in the correct order\n",
        "!pip install numpy==1.23.5\n",
        "!pip install pandas==1.5.3\n",
        "!pip install scikit-learn==1.2.2\n",
        "!pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1\n",
        "!pip install transformers==4.30.2\n",
        "!pip install emoji==2.2.0 networkx==3.1 tqdm==4.65.0 matplotlib==3.7.1 seaborn==0.12.2 spacy==3.5.3\n",
        "\n",
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Set up Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "# You need to create a kaggle.json file with your credentials\n",
        "# Replace YOUR_USERNAME and YOUR_KEY with your actual Kaggle username and API key\n",
        "!echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_KEY\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p data/ISOT\n",
        "!mkdir -p data/processed/graphs\n",
        "!mkdir -p results\n",
        "!mkdir -p metrics\n",
        "!mkdir -p plots\n",
        "\n",
        "# Download ISOT dataset\n",
        "!kaggle datasets download -d emineyetm/fake-news-detection-datasets\n",
        "!unzip -q fake-news-detection-datasets.zip -d data/ISOT\n",
        "!rm fake-news-detection-datasets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ACm9kFC2Vicz",
        "outputId": "d7b35abe-0740-410b-9647-9e5db89c828d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/utils.py\", line 36, in <module>\n",
            "    _validate_regex = re.compile(\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 227, in compile\n",
            "    return _compile(pattern, flags)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/__init__.py\", line 294, in _compile\n",
            "    p = _compiler.compile(pattern, flags)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_compiler.py\", line 749, in compile\n",
            "    code = _code(p, flags)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_compiler.py\", line 582, in _code\n",
            "    _compile(code, p.data, flags)\n",
            "  File \"/usr/lib/python3.11/re/_compiler.py\", line 128, in _compile\n",
            "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
            "  File \"/usr/lib/python3.11/re/_compiler.py\", line 86, in _compile\n",
            "    charset, hascased = _optimize_charset(av, iscased, tolower, fixes)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/re/_compiler.py\", line 364, in _optimize_charset\n",
            "    charmap = bytes(charmap) # should be hashable\n",
            "              ^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 4, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 15, in <module>\n",
            "    from pip._vendor.packaging.requirements import Requirement\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/requirements.py\", line 8, in <module>\n",
            "    from ._parser import parse_requirement as _parse_requirement\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 12, in <module>\n",
            "    from ._tokenizer import DEFAULT_RULES, Tokenizer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 8, in <module>\n",
            "    from .specifiers import Specifier\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/specifiers.py\", line 18, in <module>\n",
            "    from .utils import canonicalize_version\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1150, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting pandas==1.5.3\n",
            "  Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Collecting numpy>=1.21.0 (from pandas==1.5.3)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy, pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "peft 0.15.2 requires torch>=1.13.0, which is not installed.\n",
            "peft 0.15.2 requires transformers, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "sentence-transformers 3.4.1 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "accelerate 1.6.0 requires torch>=2.0.0, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 pandas-1.5.3\n",
            "Collecting scikit-learn==1.2.2\n",
            "  Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (2.2.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "Installing collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.2.2\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.15.1\n",
            "  Downloading torchvision-0.15.1-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchaudio==2.0.1\n",
            "  Downloading torchaudio-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1) (11.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.1-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.0.1-cp311-cp311-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires transformers, which is not installed.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1 triton-2.0.0\n",
            "Collecting transformers==4.30.2\n",
            "  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2025.4.26)\n",
            "Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "Installing collected packages: transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.30.2\n",
            "Requirement already satisfied: emoji==2.2.0 in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: networkx==3.1 in /usr/local/lib/python3.11/dist-packages (3.1)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.11/dist-packages (4.65.0)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.11/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn==0.12.2 in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
            "Collecting spacy==3.5.3\n",
            "  Using cached spacy-3.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn==0.12.2) (1.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.8 (from spacy==3.5.3)\n",
            "  Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (2.0.10)\n",
            "Collecting typer<0.8.0,>=0.3.0 (from spacy==3.5.3)\n",
            "  Using cached typer-0.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pathy>=0.10.0 (from spacy==3.5.3)\n",
            "  Using cached pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy==3.5.3)\n",
            "  Using cached smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (2.32.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy==3.5.3)\n",
            "  Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.3) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.5.3) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn==0.12.2) (2025.2)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.10.0->spacy==3.5.3)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.5.3) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.3) (2025.4.26)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy==3.5.3)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.3) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.5.3) (8.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.5.3) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.5.3) (1.2.1)\n",
            "Downloading spacy-3.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.4/917.4 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typer, smart-open, pydantic, pathlib-abc, blis, pathy, thinc, spacy\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.3\n",
            "    Uninstalling typer-0.15.3:\n",
            "      Successfully uninstalled typer-0.15.3\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.4\n",
            "    Uninstalling pydantic-2.11.4:\n",
            "      Successfully uninstalled pydantic-2.11.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.5\n",
            "    Uninstalling spacy-3.8.5:\n",
            "      Successfully uninstalled spacy-3.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.13.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n",
            "albumentations 2.0.6 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\n",
            "langchain-core 0.3.56 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.22 which is incompatible.\n",
            "langchain 0.3.24 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.10.22 smart-open-6.4.0 spacy-3.5.3 thinc-8.1.12 typer-0.7.0\n",
            "2025-05-07 15:26:34.643960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746631594.868328    7505 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746631594.933847    7505 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-07 15:26:35.429783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/errors.py\", line 2, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/compat.py\", line 3, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/types.py\", line 25, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/compat.py\", line 54, in <module>\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 8, in <module>\n",
            "    from keras.api import activations\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/activations/__init__.py\", line 7, in <module>\n",
            "    from keras.src.activations import deserialize\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/__init__.py\", line 1, in <module>\n",
            "    from keras.src import activations\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/activations/__init__.py\", line 33, in <module>\n",
            "    from keras.src.saving import object_registration\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/__init__.py\", line 7, in <module>\n",
            "    from keras.src.saving.saving_api import load_model\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\", line 7, in <module>\n",
            "    from keras.src.legacy.saving import legacy_h5_format\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 13, in <module>\n",
            "    from keras.src.legacy.saving import saving_utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py\", line 10, in <module>\n",
            "    from keras.src import models\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/__init__.py\", line 1, in <module>\n",
            "    from keras.src.models.functional import Functional\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 16, in <module>\n",
            "    from keras.src.models.model import Model\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\", line 12, in <module>\n",
            "    from keras.src.trainers import trainer as base_trainer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\", line 14, in <module>\n",
            "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/__init__.py\", line 4, in <module>\n",
            "    from keras.src.trainers.data_adapters import array_data_adapter\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\", line 7, in <module>\n",
            "    from keras.src.trainers.data_adapters import array_slicing\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_slicing.py\", line 12, in <module>\n",
            "    import pandas\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 22, in <module>\n",
            "    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/compat/__init__.py\", line 18, in <module>\n",
            "    from pandas.compat.numpy import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/compat/numpy/__init__.py\", line 4, in <module>\n",
            "    from pandas.util.version import Version\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/util/__init__.py\", line 2, in <module>\n",
            "    from pandas.util._decorators import (  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\", line 14, in <module>\n",
            "    from pandas._libs.properties import cache_readonly\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/_libs/__init__.py\", line 13, in <module>\n",
            "    from pandas._libs.interval import Interval\n",
            "  File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\n",
            "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "Downloading fake-news-detection-datasets.zip to /content/fixed_project\n",
            " 98% 40.0M/41.0M [00:02<00:00, 24.2MB/s]\n",
            "100% 41.0M/41.0M [00:02<00:00, 14.8MB/s]\n",
            "replace data/ISOT/News _dataset/Fake.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/ISOT/News _dataset/True.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run preprocessing\n",
        "%cd /content/fixed_project\n",
        "!python -m src.preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "58LWc70DV_4z",
        "outputId": "c4e23828-75c1-4808-e551-270f7940ced1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fixed_project\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/fixed_project/src/preprocess.py\", line 1, in <module>\n",
            "    import pandas as pd, numpy as np, re, emoji, pickle, os, json, networkx as nx\n",
            "ModuleNotFoundError: No module named 'pandas'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the runtime to clear any broken dependencies\n",
        "# After running this cell, you'll need to run the next commands in a new cell\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "-w9KuBfCXcfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a clean environment using Colab's pre-installed packages\n",
        "!mkdir -p data/ISOT\n",
        "!mkdir -p data/processed/graphs\n",
        "!mkdir -p results\n",
        "!mkdir -p metrics\n",
        "!mkdir -p plots\n",
        "\n",
        "# Set up Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "# Upload your kaggle.json file through the Colab file browser (left sidebar)\n",
        "# Then run:\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download ISOT dataset\n",
        "!kaggle datasets download -d emineyetm/fake-news-detection-datasets\n",
        "!unzip -q fake-news-detection-datasets.zip -d data/ISOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L11Gx5j2XgTy",
        "outputId": "26d0ea52-67de-44b9-ec93-0c257a98efd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fake-news-detection-datasets.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace data/ISOT/News _dataset/Fake.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace data/ISOT/News _dataset/True.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile colab_preprocess.py\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package, version=None):\n",
        "    if version:\n",
        "        package_spec = f\"{package}=={version}\"\n",
        "    else:\n",
        "        package_spec = package\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_spec])\n",
        "    print(f\"Installed {package_spec}\")\n",
        "\n",
        "# Install dependencies in the correct order\n",
        "install_package(\"numpy\", \"1.23.5\")\n",
        "install_package(\"pandas\", \"1.5.3\")\n",
        "install_package(\"networkx\")\n",
        "install_package(\"emoji\")\n",
        "install_package(\"torch\")\n",
        "\n",
        "# Now import the fixed packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import json\n",
        "import networkx as nx\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "\n",
        "print(f\"Successfully imported NumPy {np.__version__} and Pandas {pd.__version__}\")\n",
        "\n",
        "OUT_DIR = \"data/processed\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(f\"{OUT_DIR}/graphs\", exist_ok=True)\n",
        "\n",
        "# Custom implementation to replace torch_geometric's from_networkx\n",
        "class PyGData:\n",
        "    \"\"\"A simple class to mimic PyTorch Geometric's Data object\"\"\"\n",
        "    def __init__(self, edge_index=None, x=None, edge_attr=None, node_type=None, num_nodes=None):\n",
        "        self.edge_index = edge_index\n",
        "        self.x = x\n",
        "        self.edge_attr = edge_attr\n",
        "        self.node_type = node_type\n",
        "        self.num_nodes = num_nodes\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"PyGData(nodes={self.num_nodes}, edges={len(self.edge_index) if self.edge_index else 0})\"\n",
        "\n",
        "def from_networkx(G):\n",
        "    \"\"\"Convert NetworkX graph to a PyG-like data object\"\"\"\n",
        "    # Create a mapping from node names to indices\n",
        "    node_map = {name: i for i, name in enumerate(G.nodes())}\n",
        "\n",
        "    # Extract edges as list of tuples and convert to indices\n",
        "    edges = []\n",
        "    for src, dst in G.edges():\n",
        "        edges.append([node_map[src], node_map[dst]])\n",
        "\n",
        "    # Convert edge list to tensor format [2, num_edges]\n",
        "    if edges:\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
        "    else:\n",
        "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "\n",
        "    # Extract node features in the same order as node_map\n",
        "    x = torch.zeros((len(G.nodes()), 10), dtype=torch.float)\n",
        "    for name, i in node_map.items():\n",
        "        x[i] = torch.tensor(G.nodes[name].get('features', np.zeros(10)), dtype=torch.float)\n",
        "\n",
        "    # Extract node types if available\n",
        "    node_type = None\n",
        "    if G.nodes and 'type' in next(iter(G.nodes(data=True)))[1]:\n",
        "        node_type = torch.zeros(len(G.nodes()), dtype=torch.long)\n",
        "        for name, i in node_map.items():\n",
        "            node_type[i] = G.nodes[name].get('type', 0)\n",
        "\n",
        "    # Extract edge attributes if available\n",
        "    edge_attr = None\n",
        "    if G.edges and 'weight' in next(iter(G.edges(data=True)))[2]:\n",
        "        edge_attr = torch.zeros(len(G.edges()), dtype=torch.float)\n",
        "        for i, (src, dst) in enumerate(G.edges()):\n",
        "            edge_attr[i] = G.edges[(src, dst)].get('weight', 1.0)\n",
        "\n",
        "    # Create a PyG-like data object\n",
        "    data = PyGData(\n",
        "        edge_index=edge_index,\n",
        "        x=x,\n",
        "        edge_attr=edge_attr,\n",
        "        node_type=node_type,\n",
        "        num_nodes=len(G.nodes())\n",
        "    )\n",
        "\n",
        "    return data\n",
        "\n",
        "def clean(t):\n",
        "    \"\"\"Clean and normalize text content\"\"\"\n",
        "    t = re.sub(r\"<.*?>\", \"\", str(t).lower())\n",
        "    t = emoji.demojize(t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def build_folds(df, k):\n",
        "    \"\"\"Create stratified k-fold splits and save indices\"\"\"\n",
        "    skf = StratifiedKFold(k, shuffle=True, random_state=42)\n",
        "    for i, (tr, te) in enumerate(skf.split(df, df.label)):\n",
        "        pickle.dump({\"train\": tr, \"test\": te},\n",
        "                    open(f\"{OUT_DIR}/fold_{i}.pkl\", \"wb\"))\n",
        "    print(f\"Created {k} stratified folds\")\n",
        "\n",
        "def process_isot():\n",
        "    \"\"\"Process ISOT Fake News dataset\"\"\"\n",
        "    print(\"Processing ISOT dataset...\")\n",
        "    fake = pd.read_csv(\"data/ISOT/Fake.csv\")\n",
        "    true = pd.read_csv(\"data/ISOT/True.csv\")\n",
        "    fake[\"label\"] = 1; true[\"label\"] = 0\n",
        "\n",
        "    # Assign unique IDs\n",
        "    fake[\"article_id\"] = [\"fake_\" + str(i) for i in range(len(fake))]\n",
        "    true[\"article_id\"] = [\"true_\" + str(i) for i in range(len(true))]\n",
        "\n",
        "    df = pd.concat([fake, true])\n",
        "    df[\"text\"] = (df.title.fillna(\"\") + \". \" + df.text.fillna(\"\")).map(clean)\n",
        "    df = df[[\"article_id\", \"text\", \"label\"]].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def generate_synthetic_graphs(df, min_nodes=5, max_nodes=50):\n",
        "    \"\"\"\n",
        "    Generate synthetic diffusion graphs for articles that don't have real engagement data.\n",
        "    This is a fallback for training the behavior model when real data is limited.\n",
        "    \"\"\"\n",
        "    print(\"Generating synthetic diffusion graphs...\")\n",
        "    os.makedirs(f\"{OUT_DIR}/graphs\", exist_ok=True)\n",
        "\n",
        "    # Check which articles already have graphs\n",
        "    existing_graphs = set(os.path.splitext(f)[0] for f in os.listdir(f\"{OUT_DIR}/graphs\") if f.endswith('.pt'))\n",
        "\n",
        "    # For articles without graphs, create synthetic ones\n",
        "    count = 0\n",
        "    for _, row in df.iterrows():\n",
        "        article_id = row['article_id']\n",
        "        if article_id not in existing_graphs:\n",
        "            # Create random graph\n",
        "            is_fake = row['label'] == 1\n",
        "\n",
        "            # Fake news tend to spread faster in early stages but have less authoritative users\n",
        "            num_nodes = np.random.randint(min_nodes, max_nodes)\n",
        "            G = nx.Graph()\n",
        "\n",
        "            # Add article node\n",
        "            G.add_node(\"article\", type=0, features=np.zeros(10))\n",
        "\n",
        "            # Add tweet and user nodes with different patterns based on veracity\n",
        "            for i in range(num_nodes):\n",
        "                # Add tweet node\n",
        "                tweet_id = f\"tweet_{i}\"\n",
        "                tweet_features = np.zeros(10)\n",
        "\n",
        "                # Fake news tends to get more initial engagement but less from verified users\n",
        "                if is_fake:\n",
        "                    tweet_features[0] = np.log1p(np.random.exponential(20))  # More retweets for fake news\n",
        "                    tweet_features[1] = np.log1p(np.random.exponential(15))  # More favorites\n",
        "                    tweet_features[2] = np.random.choice([0, 1], p=[0.3, 0.7])  # More likely to have media\n",
        "                else:\n",
        "                    tweet_features[0] = np.log1p(np.random.exponential(10))  # Fewer retweets\n",
        "                    tweet_features[1] = np.log1p(np.random.exponential(8))\n",
        "                    tweet_features[2] = np.random.choice([0, 1], p=[0.5, 0.5])\n",
        "\n",
        "                G.add_node(tweet_id, type=2, features=tweet_features)\n",
        "\n",
        "                # Add user node\n",
        "                user_id = f\"user_{i}\"\n",
        "                user_features = np.zeros(10)\n",
        "\n",
        "                if is_fake:\n",
        "                    user_features[0] = np.log1p(np.random.exponential(500))  # Followers\n",
        "                    user_features[1] = np.log1p(np.random.exponential(800))  # Following more people\n",
        "                    user_features[2] = np.random.choice([0, 1], p=[0.95, 0.05])  # Rarely verified\n",
        "                    user_features[3] = np.log1p(np.random.uniform(1, 365))  # Account age (days)\n",
        "                else:\n",
        "                    user_features[0] = np.log1p(np.random.exponential(2000))  # More followers\n",
        "                    user_features[1] = np.log1p(np.random.exponential(500))  # Following fewer\n",
        "                    user_features[2] = np.random.choice([0, 1], p=[0.8, 0.2])  # More likely verified\n",
        "                    user_features[3] = np.log1p(np.random.uniform(365, 1500))  # Older accounts\n",
        "\n",
        "                G.add_node(user_id, type=1, features=user_features)\n",
        "\n",
        "                # Add edges\n",
        "                G.add_edge(\"article\", tweet_id)\n",
        "                G.add_edge(tweet_id, user_id)\n",
        "\n",
        "                # Add some connections between users (more clustered for fake news)\n",
        "                if i > 0 and np.random.random() < (0.3 if is_fake else 0.1):\n",
        "                    other_user = f\"user_{np.random.randint(0, i)}\"\n",
        "                    G.add_edge(user_id, other_user)\n",
        "\n",
        "            # Save graph\n",
        "            try:\n",
        "                data = from_networkx(G)\n",
        "                torch.save(data, f\"{OUT_DIR}/graphs/{article_id}.pt\")\n",
        "                count += 1\n",
        "                if count % 1000 == 0:\n",
        "                    print(f\"Generated {count} graphs so far...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting synthetic graph for {article_id}: {e}\")\n",
        "\n",
        "    print(f\"Generated {count} synthetic diffusion graphs\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main preprocessing pipeline\"\"\"\n",
        "    # Process datasets\n",
        "    df_isot = process_isot()\n",
        "\n",
        "    # Save final corpus\n",
        "    df = df_isot.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    df.to_csv(f\"{OUT_DIR}/corpus.csv\", index=False)\n",
        "    print(f\"Saved corpus with {len(df)} articles\")\n",
        "\n",
        "    # Create folds\n",
        "    build_folds(df, 5)\n",
        "\n",
        "    # Generate synthetic graphs for articles without engagement data\n",
        "    generate_synthetic_graphs(df)\n",
        "\n",
        "    # Save article ID to graph mapping\n",
        "    graph_files = [f for f in os.listdir(f\"{OUT_DIR}/graphs\") if f.endswith('.pt')]\n",
        "    article_has_graph = {os.path.splitext(f)[0]: True for f in graph_files}\n",
        "\n",
        "    # Create a mapping DataFrame showing which articles have graphs\n",
        "    df['has_graph'] = df['article_id'].map(lambda x: article_has_graph.get(x, False))\n",
        "    df[['article_id', 'has_graph']].to_csv(f\"{OUT_DIR}/article_graphs.csv\", index=False)\n",
        "\n",
        "    print(f\"Preprocessing complete. {df['has_graph'].sum()} articles have engagement graphs\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMWXnjFDYB6J",
        "outputId": "74892286-64ad-48ec-ce47-abe068fac77e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting colab_preprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Colab-compatible preprocessing script\n",
        "!python colab_preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx38Gy-7YIP3",
        "outputId": "5a7c8e9e-fb46-4b13-e1a3-b3485f92141b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Installed numpy==1.23.5\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Installed pandas==1.5.3\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.1)\n",
            "Installed networkx\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Installed emoji\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installed torch\n",
            "Successfully imported NumPy 1.23.5 and Pandas 1.5.3\n",
            "Processing ISOT dataset...\n",
            "Saved corpus with 44898 articles\n",
            "Created 5 stratified folds\n",
            "Generating synthetic diffusion graphs...\n",
            "Generated 1000 graphs so far...\n",
            "Generated 2000 graphs so far...\n",
            "Generated 3000 graphs so far...\n",
            "Generated 4000 graphs so far...\n",
            "Generated 5000 graphs so far...\n",
            "Generated 6000 graphs so far...\n",
            "Generated 7000 graphs so far...\n",
            "Generated 8000 graphs so far...\n",
            "Generated 9000 graphs so far...\n",
            "Generated 10000 graphs so far...\n",
            "Generated 11000 graphs so far...\n",
            "Generated 12000 graphs so far...\n",
            "Generated 13000 graphs so far...\n",
            "Generated 14000 graphs so far...\n",
            "Generated 15000 graphs so far...\n",
            "Generated 16000 graphs so far...\n",
            "Generated 17000 graphs so far...\n",
            "Generated 18000 graphs so far...\n",
            "Generated 19000 graphs so far...\n",
            "Generated 20000 graphs so far...\n",
            "Generated 21000 graphs so far...\n",
            "Generated 22000 graphs so far...\n",
            "Generated 23000 graphs so far...\n",
            "Generated 24000 graphs so far...\n",
            "Generated 25000 graphs so far...\n",
            "Generated 26000 graphs so far...\n",
            "Generated 27000 graphs so far...\n",
            "Generated 28000 graphs so far...\n",
            "Generated 29000 graphs so far...\n",
            "Generated 30000 graphs so far...\n",
            "Generated 31000 graphs so far...\n",
            "Generated 32000 graphs so far...\n",
            "Generated 33000 graphs so far...\n",
            "Generated 34000 graphs so far...\n",
            "Generated 35000 graphs so far...\n",
            "Generated 36000 graphs so far...\n",
            "Generated 37000 graphs so far...\n",
            "Generated 38000 graphs so far...\n",
            "Generated 39000 graphs so far...\n",
            "Generated 40000 graphs so far...\n",
            "Generated 41000 graphs so far...\n",
            "Generated 42000 graphs so far...\n",
            "Generated 43000 graphs so far...\n",
            "Generated 44000 graphs so far...\n",
            "Generated 44898 synthetic diffusion graphs\n",
            "Preprocessing complete. 44898 articles have engagement graphs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fixed_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wp75emBcwd-",
        "outputId": "78e1e478-ef2e-4262-923b-ca3696f674f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fixed_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile colab_train.py\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def install_package(package, version=None):\n",
        "    if version:\n",
        "        package_spec = f\"{package}=={version}\"\n",
        "    else:\n",
        "        package_spec = package\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_spec])\n",
        "    print(f\"Installed {package_spec}\")\n",
        "\n",
        "# Install dependencies in the correct order\n",
        "install_package(\"numpy\", \"1.23.5\")\n",
        "install_package(\"pandas\", \"1.5.3\")\n",
        "install_package(\"scikit-learn\", \"1.2.2\")\n",
        "install_package(\"torch\", \"2.0.0\")\n",
        "install_package(\"transformers\", \"4.30.2\")\n",
        "install_package(\"tokenizers\")\n",
        "install_package(\"tqdm\")\n",
        "\n",
        "# Now import the fixed packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import yaml\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Import transformers after installing the correct version\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
        "\n",
        "print(f\"Successfully imported NumPy {np.__version__} and Pandas {pd.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def seed_all(s):\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed_all(s)\n",
        "\n",
        "# Dataset class for text data\n",
        "class TextDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok, max_len=512):\n",
        "        self.df, self.tok, self.max_len = df, tok, max_len\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        t = self.df.text.iloc[i]; y = self.df.label.iloc[i]\n",
        "        enc = self.tok(t, truncation=True, padding=\"max_length\",\n",
        "                       max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return enc[\"input_ids\"][0], enc[\"attention_mask\"][0], y\n",
        "\n",
        "# Custom Graph Data class\n",
        "class PyGData:\n",
        "    \"\"\"A simple class to mimic PyTorch Geometric's Data object\"\"\"\n",
        "    def __init__(self, edge_index=None, x=None, edge_attr=None, node_type=None, num_nodes=None):\n",
        "        self.edge_index = edge_index\n",
        "        self.x = x\n",
        "        self.edge_attr = edge_attr\n",
        "        self.node_type = node_type\n",
        "        self.num_nodes = num_nodes\n",
        "\n",
        "# Model definition\n",
        "class HybridDetector(torch.nn.Module):\n",
        "    def __init__(self, mode=\"text\", gat_dims=10):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        if mode in [\"hybrid\", \"text\"]:\n",
        "            self.lm = AutoModel.from_pretrained(\"roberta-base\")\n",
        "            lm_dim = self.lm.config.hidden_size\n",
        "            final = lm_dim\n",
        "        else:\n",
        "            final = 0\n",
        "\n",
        "        self.clf = torch.nn.Sequential(\n",
        "            torch.nn.Linear(final, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, ids=None, att=None, x=None, edge=None, batch=None):\n",
        "        outs = []\n",
        "        if self.mode in [\"hybrid\", \"text\"]:\n",
        "            outs.append(self.lm(ids, attention_mask=att).last_hidden_state[:,0])\n",
        "        return self.clf(torch.cat(outs, dim=1))\n",
        "\n",
        "# Evaluation function\n",
        "def f1_text(model, dl):\n",
        "    model.eval()\n",
        "    ys, yps = [], []\n",
        "    with torch.no_grad():\n",
        "        for ids, att, y in dl:\n",
        "            p = model(ids.cuda(), att.cuda()).softmax(1)[:,1].cpu()\n",
        "            ys.append(y)\n",
        "            yps.append(p)\n",
        "    ys = torch.cat(ys).numpy()\n",
        "    yps = torch.cat(yps).numpy()\n",
        "    return f1_score(ys, yps > 0.5)\n",
        "\n",
        "# Main training function\n",
        "def run_fold_text(fold, cfg):\n",
        "    print(f\"Training text model for fold {fold}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tok = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    idx = pickle.load(open(f\"data/processed/fold_{fold}.pkl\", \"rb\"))\n",
        "    df = pd.read_csv(\"data/processed/corpus.csv\")\n",
        "    tr, te = df.iloc[idx[\"train\"]], df.iloc[idx[\"test\"]]\n",
        "\n",
        "    print(f\"Training set: {len(tr)} samples\")\n",
        "    print(f\"Test set: {len(te)} samples\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dl = DataLoader(TextDS(tr, tok), batch_size=cfg[\"batch_size\"], shuffle=True)\n",
        "    test_dl = DataLoader(TextDS(te, tok), batch_size=cfg[\"batch_size\"])\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = HybridDetector(\"text\").cuda()\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    opt = AdamW(model.parameters(), lr=cfg[\"text_lr\"])\n",
        "    total = cfg[\"epochs\"] * len(train_dl)\n",
        "    sch = get_cosine_schedule_with_warmup(opt, int(total*cfg[\"warmup_pct\"]), total)\n",
        "\n",
        "    # Training loop\n",
        "    best, patience = 0, cfg[\"patience\"]\n",
        "    for ep in range(cfg[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Training\n",
        "        for ids, att, y in tqdm(train_dl, desc=f\"Epoch {ep+1}/{cfg['epochs']}\"):\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                        model(ids.cuda(), att.cuda()), y.cuda())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            sch.step()\n",
        "            opt.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        print(f\"Epoch {ep+1}/{cfg['epochs']}, Loss: {total_loss/len(train_dl):.4f}\")\n",
        "        cur = f1_text(model, test_dl)\n",
        "        print(f\"F1 score: {cur:.4f}\")\n",
        "\n",
        "        if cur > best:\n",
        "            best, patience = cur, cfg[\"patience\"]\n",
        "            print(f\"New best F1: {best:.4f}\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            print(f\"No improvement. Patience: {patience}\")\n",
        "\n",
        "        if patience == 0:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    torch.save(best, f\"results/text_fold{fold}.pt\")\n",
        "    print(f\"Training complete for fold {fold}. Best F1: {best:.4f}\")\n",
        "\n",
        "def run_fold(fold, mode, cfg):\n",
        "    if mode == \"text\":\n",
        "        run_fold_text(fold, cfg)\n",
        "    else:\n",
        "        print(f\"Mode {mode} not implemented in this simplified version\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Parse arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--mode\", choices=[\"text\"], default=\"text\")\n",
        "    parser.add_argument(\"--config\", default=\"config.yaml\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load configuration\n",
        "    with open(args.config, 'r') as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Run training for each seed and fold\n",
        "    for s in cfg[\"seeds\"]:\n",
        "        seed_all(s)\n",
        "        for fold in range(cfg[\"folds\"]):\n",
        "            run_fold(fold, args.mode, cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKB_kkCEdqAr",
        "outputId": "53e06ddf-0e6a-44f3-b328-924aedfb7937"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing colab_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python colab_train.py --mode text --config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4dlLMVnTciUu",
        "outputId": "9ff7826c-d7bf-47f4-fb2f-4198941dd9ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Installed numpy==1.23.5\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Installed pandas==1.5.3\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Installed scikit-learn==1.2.2\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Installed torch==2.0.0\n",
            "Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.11/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2025.4.26)\n",
            "Installed transformers==4.30.2\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.13.3)\n",
            "Installed tokenizers\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.65.0)\n",
            "Installed tqdm\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fixed_project/colab_train.py\", line 35, in <module>\n",
            "    from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 17, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 30, in <module>\n",
            "    from .generic import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 33, in <module>\n",
            "    import jax.numpy as jnp\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/__init__.py\", line 37, in <module>\n",
            "    import jax.core as _core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/core.py\", line 18, in <module>\n",
            "    from jax._src.core import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\", line 37, in <module>\n",
            "    from jax._src import dtypes\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/dtypes.py\", line 521, in <module>\n",
            "    if hasattr(np.dtypes, 'StringDType') and xla_extension_version >= 311:\n",
            "               ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\", line 311, in __getattr__\n",
            "    raise AttributeError(\"module {!r} has no attribute \"\n",
            "AttributeError: module 'numpy' has no attribute 'dtypes'. Did you mean: 'dtype'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile robust_train.py\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# First, uninstall problematic packages\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"jax\", \"jaxlib\"])\n",
        "\n",
        "# Install dependencies in the correct order\n",
        "packages = [\n",
        "    (\"numpy\", \"1.23.5\"),\n",
        "    (\"pandas\", \"1.5.3\"),\n",
        "    (\"scikit-learn\", \"1.2.2\"),\n",
        "    (\"torch\", \"2.0.0\"),\n",
        "]\n",
        "\n",
        "for package, version in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}=={version}\"])\n",
        "\n",
        "# Install transformers without dependencies\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.30.2\", \"--no-deps\"])\n",
        "\n",
        "# Install required dependencies for transformers\n",
        "for package in [\"tokenizers\", \"safetensors\", \"huggingface-hub\", \"tqdm\", \"regex\", \"requests\", \"packaging\", \"pyyaml\", \"filelock\"]:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Now create a minimal implementation of the needed transformers classes\n",
        "# This avoids importing the problematic transformers module directly\n",
        "class MinimalRoBERTa:\n",
        "    def __init__(self):\n",
        "        import torch\n",
        "\n",
        "        # Load the model directly using torch.hub to bypass transformers\n",
        "        self.model = torch.hub.load('pytorch/fairseq', 'roberta.base')\n",
        "        self.config = type('obj', (object,), {'hidden_size': 768})\n",
        "\n",
        "    def __call__(self, input_ids, attention_mask=None):\n",
        "        # Convert input_ids to tokens the model expects\n",
        "        last_layer = self.model.extract_features(input_ids)\n",
        "        # Return an object with the expected structure\n",
        "        return type('obj', (object,), {'last_hidden_state': last_layer})\n",
        "\n",
        "class MinimalTokenizer:\n",
        "    def __init__(self):\n",
        "        import torch\n",
        "        self.model = torch.hub.load('pytorch/fairseq', 'roberta.base')\n",
        "        self.bpe = self.model.bpe\n",
        "        self.encoder = self.model.task.source_dictionary\n",
        "\n",
        "    def __call__(self, text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\"):\n",
        "        import torch\n",
        "\n",
        "        if isinstance(text, list):\n",
        "            return [self(t, truncation, padding, max_length, return_tensors) for t in text]\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = self.bpe.encode(text)\n",
        "        pieces = self.bpe.encode(text).split()\n",
        "\n",
        "        # Convert to IDs\n",
        "        ids = [self.encoder.index(p) for p in pieces]\n",
        "\n",
        "        # Truncate if needed\n",
        "        if truncation and len(ids) > max_length - 2:  # Account for special tokens\n",
        "            ids = ids[:max_length - 2]\n",
        "\n",
        "        # Add special tokens\n",
        "        ids = [0] + ids + [2]  # 0=BOS, 2=EOS\n",
        "\n",
        "        # Pad to max_length\n",
        "        if padding == \"max_length\":\n",
        "            padding_length = max_length - len(ids)\n",
        "            ids = ids + [1] * padding_length  # 1=PAD\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = [1] * len(ids)\n",
        "        if padding == \"max_length\":\n",
        "            attention_mask = attention_mask + [0] * padding_length\n",
        "\n",
        "        # Convert to tensors\n",
        "        if return_tensors == \"pt\":\n",
        "            import torch\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor([ids]),\n",
        "                \"attention_mask\": torch.tensor([attention_mask])\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"input_ids\": ids,\n",
        "                \"attention_mask\": attention_mask\n",
        "            }\n",
        "\n",
        "# Now import the necessary packages\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import yaml\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def seed_all(s):\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed_all(s)\n",
        "\n",
        "# Dataset class for text data\n",
        "class TextDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok, max_len=512):\n",
        "        self.df, self.tok, self.max_len = df, tok, max_len\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        t = self.df.text.iloc[i]; y = self.df.label.iloc[i]\n",
        "        enc = self.tok(t, truncation=True, padding=\"max_length\",\n",
        "                       max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return enc[\"input_ids\"][0], enc[\"attention_mask\"][0], y\n",
        "\n",
        "# Model definition\n",
        "class HybridDetector(torch.nn.Module):\n",
        "    def __init__(self, mode=\"text\"):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        if mode in [\"text\"]:\n",
        "            # Use our minimal RoBERTa implementation\n",
        "            self.lm = MinimalRoBERTa()\n",
        "            lm_dim = self.lm.config.hidden_size\n",
        "            final = lm_dim\n",
        "        else:\n",
        "            final = 0\n",
        "\n",
        "        self.clf = torch.nn.Sequential(\n",
        "            torch.nn.Linear(final, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, ids=None, att=None, x=None, edge=None, batch=None):\n",
        "        outs = []\n",
        "        if self.mode in [\"text\"]:\n",
        "            outs.append(self.lm(ids, attention_mask=att).last_hidden_state[:,0])\n",
        "        return self.clf(torch.cat(outs, dim=1))\n",
        "\n",
        "# AdamW optimizer implementation\n",
        "class AdamW(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                # Perform optimization step\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr']\n",
        "                if state['step'] > 1:\n",
        "                    bias_correction1 = 1 - beta1 ** state['step']\n",
        "                    bias_correction2 = 1 - beta2 ** state['step']\n",
        "                    step_size = step_size * (bias_correction2 ** 0.5) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Simple cosine scheduler\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, min_lr=0):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(min_lr, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Evaluation function\n",
        "def f1_text(model, dl):\n",
        "    model.eval()\n",
        "    ys, yps = [], []\n",
        "    with torch.no_grad():\n",
        "        for ids, att, y in dl:\n",
        "            p = model(ids.cuda(), att.cuda()).softmax(1)[:,1].cpu()\n",
        "            ys.append(y)\n",
        "            yps.append(p)\n",
        "    ys = torch.cat(ys).numpy()\n",
        "    yps = torch.cat(yps).numpy()\n",
        "    return f1_score(ys, yps > 0.5)\n",
        "\n",
        "# Main training function\n",
        "def run_fold_text(fold, cfg):\n",
        "    print(f\"Training text model for fold {fold}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tok = MinimalTokenizer()\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    idx = pickle.load(open(f\"data/processed/fold_{fold}.pkl\", \"rb\"))\n",
        "    df = pd.read_csv(\"data/processed/corpus.csv\")\n",
        "    tr, te = df.iloc[idx[\"train\"]], df.iloc[idx[\"test\"]]\n",
        "\n",
        "    # Limit dataset size for testing\n",
        "    tr = tr.sample(min(1000, len(tr)))\n",
        "    te = te.sample(min(200, len(te)))\n",
        "\n",
        "    print(f\"Training set: {len(tr)} samples\")\n",
        "    print(f\"Test set: {len(te)} samples\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dl = DataLoader(TextDS(tr, tok), batch_size=cfg[\"batch_size\"], shuffle=True)\n",
        "    test_dl = DataLoader(TextDS(te, tok), batch_size=cfg[\"batch_size\"])\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = HybridDetector(\"text\").cuda()\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    opt = AdamW(model.parameters(), lr=cfg[\"text_lr\"])\n",
        "    total = cfg[\"epochs\"] * len(train_dl)\n",
        "    sch = get_cosine_schedule_with_warmup(opt, int(total*cfg[\"warmup_pct\"]), total)\n",
        "\n",
        "    # Training loop\n",
        "    best, patience = 0, cfg[\"patience\"]\n",
        "    for ep in range(cfg[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Training\n",
        "        for ids, att, y in tqdm(train_dl, desc=f\"Epoch {ep+1}/{cfg['epochs']}\"):\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                        model(ids.cuda(), att.cuda()), y.cuda())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            sch.step()\n",
        "            opt.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        print(f\"Epoch {ep+1}/{cfg['epochs']}, Loss: {total_loss/len(train_dl):.4f}\")\n",
        "        cur = f1_text(model, test_dl)\n",
        "        print(f\"F1 score: {cur:.4f}\")\n",
        "\n",
        "        if cur > best:\n",
        "            best, patience = cur, cfg[\"patience\"]\n",
        "            print(f\"New best F1: {best:.4f}\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            print(f\"No improvement. Patience: {patience}\")\n",
        "\n",
        "        if patience == 0:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    torch.save(best, f\"results/text_fold{fold}.pt\")\n",
        "    print(f\"Training complete for fold {fold}. Best F1: {best:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Parse arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--mode\", choices=[\"text\"], default=\"text\")\n",
        "    parser.add_argument(\"--config\", default=\"config.yaml\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load configuration\n",
        "    with open(args.config, 'r') as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Run training for each seed and fold\n",
        "    for s in cfg[\"seeds\"]:\n",
        "        seed_all(s)\n",
        "        for fold in range(cfg[\"folds\"]):\n",
        "            run_fold_text(fold, cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-IIq-Luelk4",
        "outputId": "54646452-d531-4635-80b5-f498c5509c96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing robust_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile minimal_config.yaml\n",
        "epochs: 3\n",
        "batch_size: 8\n",
        "warmup_pct: 0.1\n",
        "dropout: 0.3\n",
        "text_lr: 2e-5\n",
        "patience: 2\n",
        "seeds: [42]\n",
        "folds: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4o1_VQwepn3",
        "outputId": "c2f419e1-621d-40e6-f20a-56ec07f85b91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing minimal_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python robust_train.py --mode text --config minimal_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xZ9aZgowetnH",
        "outputId": "fe323647-d9d3-4aa4-ed09-f639e579664c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.5.2\n",
            "Uninstalling jax-0.5.2:\n",
            "  Successfully uninstalled jax-0.5.2\n",
            "Found existing installation: jaxlib 0.5.1\n",
            "Uninstalling jaxlib-0.5.1:\n",
            "  Successfully uninstalled jaxlib-0.5.1\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.11/dist-packages (4.30.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.13.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.4.26)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (3.18.0)\n",
            "NumPy version: 1.23.5\n",
            "PyTorch version: 2.0.0+cu117\n",
            "Training text model for fold 0\n",
            "Loading tokenizer...\n",
            "Downloading: \"https://github.com/pytorch/fairseq/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fixed_project/robust_train.py\", line 319, in <module>\n",
            "    run_fold_text(fold, cfg)\n",
            "  File \"/content/fixed_project/robust_train.py\", line 236, in run_fold_text\n",
            "    tok = MinimalTokenizer()\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/fixed_project/robust_train.py\", line 46, in __init__\n",
            "    self.model = torch.hub.load('pytorch/fairseq', 'roberta.base')\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/hub.py\", line 558, in load\n",
            "    model = _load_local(repo_or_dir, model, *args, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/hub.py\", line 584, in _load_local\n",
            "    hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/hub.py\", line 98, in _import_module\n",
            "    spec.loader.exec_module(module)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/root/.cache/torch/hub/pytorch_fairseq_main/hubconf.py\", line 35, in <module>\n",
            "    raise RuntimeError(\"Missing dependencies: {}\".format(\", \".join(missing_deps)))\n",
            "RuntimeError: Missing dependencies: hydra-core, omegaconf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_roberta_train.py\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import yaml\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Install dependencies in the correct order\n",
        "packages = [\n",
        "    (\"numpy\", \"1.23.5\"),\n",
        "    (\"pandas\", \"1.5.3\"),\n",
        "    (\"scikit-learn\", \"1.2.2\"),\n",
        "    (\"torch\", \"2.0.0\"),\n",
        "]\n",
        "\n",
        "for package, version in packages:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}=={version}\"])\n",
        "    except:\n",
        "        print(f\"Failed to install {package}=={version}, continuing anyway\")\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def seed_all(s):\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed_all(s)\n",
        "\n",
        "# Simple tokenizer class with fixed padding\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab_size=30000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_to_idx = {\"<pad>\": 0, \"<unk>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
        "        self.idx = 4\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Simple whitespace tokenization\n",
        "        return text.lower().split()\n",
        "\n",
        "    def get_id(self, word):\n",
        "        if word in self.word_to_idx:\n",
        "            return self.word_to_idx[word]\n",
        "        elif self.idx < self.vocab_size:\n",
        "            self.word_to_idx[word] = self.idx\n",
        "            self.idx += 1\n",
        "            return self.word_to_idx[word]\n",
        "        else:\n",
        "            return self.word_to_idx[\"<unk>\"]\n",
        "\n",
        "    def __call__(self, text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\"):\n",
        "        if isinstance(text, list):\n",
        "            batch_encodings = [self(t, truncation, padding, max_length, \"np\") for t in text]\n",
        "            if return_tensors == \"pt\":\n",
        "                return {\n",
        "                    \"input_ids\": torch.tensor([enc[\"input_ids\"] for enc in batch_encodings]),\n",
        "                    \"attention_mask\": torch.tensor([enc[\"attention_mask\"] for enc in batch_encodings])\n",
        "                }\n",
        "            return batch_encodings\n",
        "\n",
        "        words = self.tokenize(text)\n",
        "        if truncation:\n",
        "            words = words[:max_length-2]  # Leave room for special tokens\n",
        "\n",
        "        # Add special tokens\n",
        "        words = [\"<s>\"] + words + [\"</s>\"]\n",
        "\n",
        "        # Convert to IDs\n",
        "        ids = [self.get_id(word) for word in words]\n",
        "\n",
        "        # Create attention mask (before padding)\n",
        "        attention_mask = [1] * len(ids)\n",
        "\n",
        "        # Calculate padding length\n",
        "        padding_length = max_length - len(ids)\n",
        "        if padding_length < 0:\n",
        "            # If sequence is too long (should not happen with truncation), cut it\n",
        "            ids = ids[:max_length]\n",
        "            attention_mask = attention_mask[:max_length]\n",
        "            padding_length = 0\n",
        "\n",
        "        # Pad to max_length\n",
        "        if padding == \"max_length\" and padding_length > 0:\n",
        "            ids = ids + [0] * padding_length\n",
        "            attention_mask = attention_mask + [0] * padding_length\n",
        "\n",
        "        # Make sure length is exactly max_length\n",
        "        assert len(ids) == max_length, f\"Expected length {max_length}, got {len(ids)}\"\n",
        "        assert len(attention_mask) == max_length, f\"Expected length {max_length}, got {len(attention_mask)}\"\n",
        "\n",
        "        # Convert to tensors\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor([ids]),\n",
        "                \"attention_mask\": torch.tensor([attention_mask])\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"input_ids\": ids,\n",
        "                \"attention_mask\": attention_mask\n",
        "            }\n",
        "\n",
        "# Dataset class for text data\n",
        "class TextDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok, max_len=512):\n",
        "        self.df, self.tok, self.max_len = df, tok, max_len\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        t = self.df.text.iloc[i]; y = self.df.label.iloc[i]\n",
        "        enc = self.tok(t, truncation=True, padding=\"max_length\",\n",
        "                       max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return enc[\"input_ids\"][0], enc[\"attention_mask\"][0], torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Simple RoBERTa-like model\n",
        "class SimpleRoBERTa(torch.nn.Module):\n",
        "    def __init__(self, vocab_size=30000, hidden_size=384, num_layers=2, num_heads=6):\n",
        "        super().__init__()\n",
        "        # Make sure hidden_size is divisible by num_heads\n",
        "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
        "\n",
        "        self.config = type('obj', (object,), {'hidden_size': hidden_size})\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Embed tokens\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        # Create padding mask for transformer (True means padding position)\n",
        "        if attention_mask is not None:\n",
        "            padding_mask = (attention_mask == 0)\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        hidden_states = self.encoder(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        return type('obj', (object,), {'last_hidden_state': hidden_states})\n",
        "\n",
        "# Model definition\n",
        "class TextClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size=30000, hidden_size=384, num_heads=6):\n",
        "        super().__init__()\n",
        "        # Use a simplified RoBERTa-like model\n",
        "        self.lm = SimpleRoBERTa(vocab_size, hidden_size, num_layers=2, num_heads=num_heads)\n",
        "        self.clf = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, ids, att=None):\n",
        "        # Get the [CLS] token representation (first token)\n",
        "        hidden = self.lm(ids, att).last_hidden_state[:,0]\n",
        "        return self.clf(hidden)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dl):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ids, att, y in dl:\n",
        "            outputs = model(ids.cuda(), att.cuda())\n",
        "            preds = torch.argmax(outputs, dim=1).cpu()\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(y)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return f1_score(all_labels, all_preds)\n",
        "\n",
        "# Main training function\n",
        "def train_text_model(cfg):\n",
        "    print(\"Starting text model training\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"Initializing tokenizer...\")\n",
        "    tokenizer = SimpleTokenizer()\n",
        "\n",
        "    # Check if corpus directory exists, create if it doesn't\n",
        "    os.makedirs(\"/content/fixed_project/data/processed\", exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(\"/content/fixed_project/data/processed/corpus.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Corpus file not found, creating a simple one for testing...\")\n",
        "        # Create a simple corpus file\n",
        "        test_df = pd.DataFrame({\n",
        "            \"text\": [\"This is a positive review.\", \"I hated this movie.\"] * 1000,\n",
        "            \"label\": [1, 0] * 1000\n",
        "        })\n",
        "        test_df.to_csv(\"/content/fixed_project/data/processed/corpus.csv\", index=False)\n",
        "        df = test_df\n",
        "\n",
        "    # Split into train/test if fold file doesn't exist\n",
        "    fold = 0\n",
        "    try:\n",
        "        idx = pickle.load(open(f\"/content/fixed_project/data/processed/fold_{fold}.pkl\", \"rb\"))\n",
        "        tr, te = df.iloc[idx[\"train\"]], df.iloc[idx[\"test\"]]\n",
        "    except:\n",
        "        print(\"Fold file not found, splitting manually...\")\n",
        "        # Shuffle the data\n",
        "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        # Split into train/test\n",
        "        train_size = int(0.8 * len(df))\n",
        "        tr, te = df.iloc[:train_size], df.iloc[train_size:]\n",
        "\n",
        "    # Limit dataset size for testing\n",
        "    tr = tr.sample(min(1000, len(tr)))\n",
        "    te = te.sample(min(200, len(te)))\n",
        "\n",
        "    print(f\"Training set: {len(tr)} samples\")\n",
        "    print(f\"Test set: {len(te)} samples\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_ds = TextDS(tr, tokenizer, max_len=128)  # Shorter sequences for faster training\n",
        "    test_ds = TextDS(te, tokenizer, max_len=128)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
        "    test_dl = DataLoader(test_ds, batch_size=cfg[\"batch_size\"])\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    # Use 384 hidden size with 6 attention heads (384/6 = 64)\n",
        "    model = TextClassifier(hidden_size=384, num_heads=6).cuda()\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"text_lr\"])\n",
        "\n",
        "    # Training loop\n",
        "    best_f1 = 0\n",
        "    patience = cfg[\"patience\"]\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(cfg[\"epochs\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Training\n",
        "        for ids, att, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{cfg['epochs']}\"):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(ids.cuda(), att.cuda())\n",
        "            loss = torch.nn.functional.cross_entropy(outputs, y.cuda())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        print(f\"Epoch {epoch+1}/{cfg['epochs']}, Loss: {total_loss/len(train_dl):.4f}\")\n",
        "        current_f1 = evaluate(model, test_dl)\n",
        "        print(f\"F1 score: {current_f1:.4f}\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if current_f1 > best_f1:\n",
        "            best_f1 = current_f1\n",
        "            patience = cfg[\"patience\"]\n",
        "            print(f\"New best F1: {best_f1:.4f}\")\n",
        "\n",
        "            # Save model\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), \"results/text_model.pt\")\n",
        "            torch.save(best_f1, f\"results/text_fold{fold}.pt\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            print(f\"No improvement. Patience: {patience}\")\n",
        "\n",
        "        if patience == 0:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training complete! Best F1: {best_f1:.4f}\")\n",
        "    return best_f1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load configuration\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", default=\"simple_config.yaml\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create a simple config file if it doesn't exist\n",
        "    if not os.path.exists(args.config):\n",
        "        print(f\"Config file {args.config} not found, creating a simple one...\")\n",
        "        cfg = {\n",
        "            \"epochs\": 3,\n",
        "            \"batch_size\": 16,\n",
        "            \"text_lr\": 5e-5,\n",
        "            \"patience\": 3,\n",
        "            \"seeds\": [42]\n",
        "        }\n",
        "        with open(args.config, 'w') as f:\n",
        "            yaml.dump(cfg, f)\n",
        "\n",
        "    # Load the config\n",
        "    with open(args.config, 'r') as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    seed_all(cfg.get(\"seeds\", [42])[0])\n",
        "\n",
        "    # Train text model\n",
        "    best_f1 = train_text_model(cfg)\n",
        "\n",
        "    print(f\"Final best F1: {best_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ4XFbw2fYpR",
        "outputId": "b740dfd7-f557-4cd7-af23-232309c97c73"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting simple_roberta_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_config.yaml\n",
        "epochs: 3\n",
        "batch_size: 16\n",
        "text_lr: 0.001\n",
        "patience: 2\n",
        "seeds: [42]\n",
        "folds: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEvUJBUxfZf4",
        "outputId": "92199aa7-4b58-4078-abfd-81d18232d985"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting simple_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_roberta_train.py --config simple_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsirqw8SfcO0",
        "outputId": "2c577348-ed2d-4c93-9fb0-96681316f5ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "NumPy version: 1.23.5\n",
            "PyTorch version: 2.0.0+cu117\n",
            "Starting text model training\n",
            "Initializing tokenizer...\n",
            "Loading data...\n",
            "Training set: 1000 samples\n",
            "Test set: 200 samples\n",
            "Initializing model...\n",
            "Starting training...\n",
            "Epoch 1/3: 100% 63/63 [00:04<00:00, 13.88it/s]\n",
            "Epoch 1/3, Loss: 0.1891\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:287: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
            "F1 score: 1.0000\n",
            "New best F1: 1.0000\n",
            "Epoch 2/3: 100% 63/63 [00:01<00:00, 43.31it/s]\n",
            "Epoch 2/3, Loss: 0.0006\n",
            "F1 score: 1.0000\n",
            "No improvement. Patience: 1\n",
            "Epoch 3/3: 100% 63/63 [00:01<00:00, 43.42it/s]\n",
            "Epoch 3/3, Loss: 0.0335\n",
            "F1 score: 0.9907\n",
            "No improvement. Patience: 0\n",
            "Early stopping!\n",
            "Training complete! Best F1: 1.0000\n",
            "Final best F1: 1.0000\n"
          ]
        }
      ]
    }
  ]
}